{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34caf4df-af4c-4927-8b96-1264193f62fd",
   "metadata": {},
   "source": [
    "## Hệ  thống phân tích và so sánh nội dung WIKIPEDIA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580af16d-bc1e-41e5-bf34-fd6625b5a5c9",
   "metadata": {},
   "source": [
    "- Thu thập nội dung Wikipedia từ hai phiên bản ngôn ngữ khác nhau , trường hợp cụ thể trong dự án là Chiến tranh Việt Nam(tiếng Việt và tiếng Anh)\n",
    "- Phân tích nội dung:\n",
    "  - So sánh 2 bài viết về mặt nội dung, phân tích từ ngữ, giọng văn,độ dài và các yếu tố liên quan\n",
    "  - Tìm ra điểm tương đồng và khác biệt\n",
    "- Phân tích cấu trúc:\n",
    "  - So sánh cấu trúc bài viết, bao gồm cách sắp xếp các mục, độ dài của các phần, số lượng liên kết và trích dẫn.\n",
    "- Tạo biểu đồ trực quan:\n",
    "  - Tạo ra các trực quan hóa (visualizations) dưới dạng biểu đồ hoặc hình ảnh minh họa về các thông tin phân tích, giúp người dùng dễ dàng nhận thấy sự khác biệt hoặc tương đồng.\n",
    "- Tạo report :\n",
    "  - xuất ra báo cáo tổng hợp dưới dạng JSON và HTML, bao gồm các phần nội dung đã phân tích và các biểu đồ trực quan để cung cấp cái nhìn sâu sắc về sự khác nhau giữa các phiên bản ngôn ngữ của bài viết."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "815ff39c-a699-4056-ac48-cd3c58f50161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import logging\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a15ad3f-3428-46c2-aade-464d20f55164",
   "metadata": {},
   "source": [
    "- requests: Dùng để gửi các yêu cầu HTTP trong Python, giúp lấy nội dung từ một URL\n",
    "- BeautifulSoup (từ bs4): Thư viện để phân tích cú pháp HTML và XML.\n",
    "- re: Đây là thư viện về biểu thức chính quy (regular expressions) của Python, được dùng để tìm kiếm và xử lý các chuỗi dựa trên mẫu.\n",
    "- collections.Counter: Là một dạng từ điển đặc biệt để đếm các đối tượng hashable\n",
    "- nltk (Natural Language Toolkit): Thư viện xử lý ngôn ngữ tự nhiên\n",
    "    -  word_tokenize và sent_tokenize: Tách văn bản thành từ hoặc câu.\n",
    "    -  stopwords: Danh sách các từ phổ biến (ví dụ: \"the\", \"is\") thường bị loại bỏ trong quá trình xử lý văn bản.\n",
    "- matplotlib.pyplot: Thư viện phổ biến để vẽ đồ thị\n",
    "- pandas: Thư viện dùng để phân tích và xử lý dữ liệu dạng bảng\n",
    "- numpy: Thư viện cho các thao tác toán học, đặc biệt là với các mảng (arrays) và tính toán số học.\n",
    "- TextBlob: Thư viện xử lý văn bản, cung cấp API đơn giản cho các tác vụ như phân tích cảm xúc và trích xuất cụm danh từ.\n",
    "- seaborn: Thư viện vẽ biểu đồ thống kê, xây dựng trên matplotlib, tạo ra các đồ thị trực quan đẹp mắt và có tính thống kê cao.\n",
    "- networkx: Thư viện dùng để tạo, thao tác và nghiên cứu các mạng\n",
    "- TfidfVectorizer: Thuộc scikit-learn, dùng để chuyển đổi một tập hợp các tài liệu văn bản thành ma trận của các đặc trưng TF-ID\n",
    "- logging: Thư viện tiêu chuẩn của Python để ghi lại và quản lý các thông điệp log\n",
    "- json: Thư viện hỗ trợ xử lý dữ liệu JSON\n",
    "- os: Thư viện tương tác với hệ điều hành"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a65ffee0-1f5d-4d73-8d1a-8dfc35f1b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cấu hình logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('wikipedia_analysis.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b3b72b-bc0b-4ff1-80aa-bdb331b94528",
   "metadata": {},
   "source": [
    "- Hàm này dùng để cấu hình hệ thống ghi log (logging system). Nó đặt các cài đặt cơ bản cho các thông điệp log như mức độ (level), định dạng (format) và nơi lưu trữ (handlers).\n",
    "- level=logging.INFO  : Mức độ thông báo của log,những thông báo có mức độ từ Info trở lên sẽ được báo cáo\n",
    "- handlers : Danh sách các nơi mà log sẽ được gửi đến"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0eef840-22b6-4ebe-b64a-08dbdd8acdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47bc1ba1-3553-417a-ae75-5ca9a53ad5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikipediaAnalysisSystem:\n",
    "    def __init__(self, output_dir='output'):\n",
    "        \"\"\"Khởi tạo hệ thống phân tích Wikipedia\"\"\"\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Tải các resource cần thiết từ NLTK\n",
    "        try:\n",
    "            nltk.download('punkt')\n",
    "            nltk.download('stopwords')\n",
    "            logger.info(\"NLTK resources downloaded successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error downloading NLTK resources: {e}\")\n",
    "\n",
    "        # Khởi tạo stopwords\n",
    "        self.vietnamese_stopwords = set([\n",
    "            \"của\", \"và\", \"các\", \"có\", \"là\", \"được\", \"trong\", \"đã\", \"cho\", \"những\",\n",
    "            \"với\", \"này\", \"về\", \"như\", \"để\", \"từ\", \"theo\", \"tại\", \"một\", \"không\",\n",
    "            \"còn\", \"bị\", \"khi\", \"làm\", \"nhưng\", \"có thể\", \"vì\", \"do\", \"người\", \"cũng\",\n",
    "            \"sau\", \"phải\", \"trên\", \"đến\", \"đang\", \"ra\", \"thì\", \"nên\", \"việc\", \"cần\"\n",
    "        ])\n",
    "        self.english_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c743295-03a0-4a2f-b295-656b581a3dd2",
   "metadata": {},
   "source": [
    "- Định nghĩa lớp khởi tạo\n",
    "- Tải các tài nguyên cần thiét từ NLTK:\n",
    "    - punkt là một gói từ NLTK giúp tách (tokenize) văn bản thành câu và từ.\n",
    "    - stopwords là một gói cung cấp danh sách các từ dừng (stopwords) trong nhiều ngôn ngữ. Stopwords là những từ phổ biến như \"là\", \"và\", \"của\" thường bị loại bỏ trong phân tích văn bản.\n",
    "    - Khởi tạo một stopwords tiếng việtboiwr vì nltk chưa hỗ trợ\n",
    " \n",
    "- Tóm lại : Lớp WikipediaAnalysisSystem này giúp khởi tạo một hệ thống phân tích Wikipedia. Nó thiết lập thư mục đầu ra để lưu trữ kết quả, tải các tài nguyên cần thiết từ NLTK để phân tích ngôn ngữ, và khởi tạo danh sách các từ dừng (stopwords) cho tiếng Việt và tiếng Anh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7afba0b-3dfd-4099-9948-4d764ad8fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikipediaArticle:\n",
    "    def __init__(self, url, language):\n",
    "        \"\"\"Khởi tạo đối tượng bài viết Wikipedia\"\"\"\n",
    "        self.url = url\n",
    "        self.language = language\n",
    "        self.soup = None\n",
    "        self.content = None\n",
    "        self.title = None\n",
    "        self.metadata = {}\n",
    "\n",
    "    def fetch_content(self):\n",
    "        \"\"\"Lấy nội dung bài viết từ Wikipedia\"\"\"\n",
    "        try:\n",
    "            response = requests.get(self.url)\n",
    "            response.raise_for_status()\n",
    "            self.soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            self.content = self.soup.find(id=\"mw-content-text\").get_text()\n",
    "            self.title = self.soup.find(id=\"firstHeading\").get_text()\n",
    "            logger.info(f\"Successfully fetched content for {self.title}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching content from {self.url}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def extract_metadata(self):\n",
    "        \"\"\"Trích xuất metadata của bài viết\"\"\"\n",
    "        try:\n",
    "            self.metadata = {\n",
    "                'title': self.title,\n",
    "                'url': self.url,\n",
    "                'language': self.language,\n",
    "                'last_modified': self.soup.find(id=\"footer-info-lastmod\").get_text(),\n",
    "                'word_count': len(self.content.split()),\n",
    "                'fetch_time': datetime.now().isoformat()\n",
    "            }\n",
    "            return self.metadata\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting metadata: {e}\")\n",
    "            return {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da953b3d-8593-413c-8701-d9a27334a764",
   "metadata": {},
   "source": [
    "- Khởi tạo đối tượng WikipediaArticle với các thuộc tính:\n",
    "  -  self.url: Lưu trữ URL của bài viết Wikipedia.\n",
    "  -  self.language: Lưu trữ ngôn ngữ của bài viết.\n",
    "  -  self.soup: Đây sẽ là đối tượng BeautifulSoup sau khi lấy nội dung HTML từ Wikipedia.\n",
    "  -  self.content: Lưu trữ nội dung chính của bài viết.\n",
    "  -  self.title: Lưu trữ tiêu đề của bài viết.\n",
    "  -  self.metadata: Đây là dictionary để lưu trữ các thông tin metadata (siêu dữ liệu) về bài viết.\n",
    "- Hàm fetch_content :\n",
    "  - requests.get(self.url): Gửi một yêu cầu HTTP GET đến URL của bài viết Wikipedia để lấy nội dung HTML.\n",
    "  - response.raise_for_status(): Kiểm tra xem yêu cầu có thành công hay không (nếu có lỗi HTTP, sẽ phát sinh lỗi và không tiếp tục thực thi các dòng mã sau đó).\n",
    "  - self.soup = BeautifulSoup(response.content, 'html.parser'): Tạo một đối tượng BeautifulSoup từ nội dung HTML đã nhận được từ Wikipedia, giúp phân tích cú pháp trang web.\n",
    "  - self.content = self.soup.find(id=\"mw-content-text\").get_text():\n",
    "    - Tìm thẻ HTML có id=\"mw-content-text\", đây là nơi chứa nội dung chính của bài viết Wikipedia.\n",
    "    - .get_text(): Lấy toàn bộ văn bản (text) từ thẻ này và lưu vào self.content.\n",
    "  - Tìm thẻ có id=\"firstHeading\" để lấy tiêu đề của bài viết.\n",
    "  - logger.info(f\"Successfully fetched content for {self.title}\"): Ghi lại log khi lấy nội dung thành công\n",
    "  - return True: Trả về True nếu việc lấy nội dung thành công.\n",
    "  -  Nếu có lỗi xảy ra trong quá trình lấy nội dung, chương trình sẽ bắt lỗi và ghi lại thông báo lỗi qua logger. Trả về False nếu không thể lấy nội dung bài viết.\n",
    "- Hàm extract_metadat :\n",
    "  - self.metadata: Trích xuất các thông tin metadata từ bài viết và lưu vào dictionary self.metadata với các trường:\n",
    "    - title: Tiêu đề của bài viết.\n",
    "    - url: Đường dẫn URL của bài viết.\n",
    "    - language: Ngôn ngữ của bài viết.\n",
    "    - last_modified: Thông tin về lần cuối cùng bài viết được chỉnh sửa, được lấy từ thẻ HTML có id=\"footer-info-lastmod\".\n",
    "    - word_count: Số lượng từ trong bài viết (được tính bằng cách tách từ trong self.content và đếm số phần tử).\n",
    "    - fetch_time: Thời gian mà nội dung bài viết được lấy về, sử dụng hàm datetime.now().isoformat() để lấy thời gian hiện tại theo định dạng ISO 8601.\n",
    "  - return self.metadata: Trả về dictionary chứa metadata của bài viết.\n",
    "\n",
    "  - except Exception as e: Nếu có lỗi xảy ra khi trích xuất metadata, chương trình sẽ ghi lại lỗi và trả về một dictionary rỗng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3023fc4-52d9-4cf4-ba12-45645964f60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentAnalyzer:\n",
    "    def __init__(self, article_vi, article_en, system):\n",
    "        \"\"\"Khởi tạo bộ phân tích nội dung\"\"\"\n",
    "        self.article_vi = article_vi\n",
    "        self.article_en = article_en\n",
    "        self.system = system\n",
    "\n",
    "    def analyze_text_content(self):\n",
    "        \"\"\"Phân tích nội dung văn bản\"\"\"\n",
    "        results = {\n",
    "            'word_frequency': self._analyze_word_frequency(),\n",
    "            'sentiment': self._analyze_sentiment(),\n",
    "            'readability': self._analyze_readability(),\n",
    "            'complexity': self._analyze_complexity(),\n",
    "            'named_entities': self._extract_named_entities(),\n",
    "            'temporal_references': self._analyze_temporal_references()\n",
    "        }\n",
    "        return results\n",
    "\n",
    "    def _analyze_word_frequency(self):\n",
    "        \"\"\"Phân tích tần suất từ\"\"\"\n",
    "        def get_frequency(text, stopwords):\n",
    "            words = word_tokenize(text.lower())\n",
    "            words = [w for w in words if w.isalnum() and w not in stopwords]\n",
    "            return Counter(words).most_common(20)\n",
    "\n",
    "        return {\n",
    "            'vietnamese': get_frequency(self.article_vi.content, self.system.vietnamese_stopwords),\n",
    "            'english': get_frequency(self.article_en.content, self.system.english_stopwords)\n",
    "        }\n",
    "\n",
    "    def _analyze_sentiment(self):\n",
    "        \"\"\"Phân tích tình cảm/quan điểm\"\"\"\n",
    "        def get_sentiment(text):\n",
    "            blob = TextBlob(text)\n",
    "            return {\n",
    "                'polarity': blob.sentiment.polarity,\n",
    "                'subjectivity': blob.sentiment.subjectivity\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            'vietnamese': get_sentiment(self.article_vi.content),\n",
    "            'english': get_sentiment(self.article_en.content)\n",
    "        }\n",
    "\n",
    "    def _analyze_readability(self):\n",
    "        \"\"\"Phân tích độ dễ đọc\"\"\"\n",
    "        def calculate_readability(text):\n",
    "            sentences = sent_tokenize(text)\n",
    "            words = word_tokenize(text)\n",
    "            return {\n",
    "                'sentence_count': len(sentences),\n",
    "                'word_count': len(words),\n",
    "                'avg_sentence_length': len(words) / len(sentences) if sentences else 0,\n",
    "                'avg_word_length': sum(len(word) for word in words) / len(words) if words else 0\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            'vietnamese': calculate_readability(self.article_vi.content),\n",
    "            'english': calculate_readability(self.article_en.content)\n",
    "        }\n",
    "        \n",
    "    def _analyze_complexity(self):\n",
    "        \"\"\"Phân tích độ phức tạp của văn bản\"\"\"\n",
    "        def calculate_complexity(text):\n",
    "            words = word_tokenize(text)\n",
    "            complex_words = len([w for w in words if len(w) > 6])\n",
    "            \n",
    "            sentences = sent_tokenize(text)\n",
    "            complex_word_ratio = complex_words / len(words) if words else 0\n",
    "            avg_sentence_length = len(words) / len(sentences) if sentences else 0\n",
    "            \n",
    "            return {\n",
    "                'complex_words_count': complex_words,\n",
    "                'complex_word_ratio': complex_word_ratio,\n",
    "                'avg_sentence_length': avg_sentence_length,\n",
    "                'gunning_fog_index': 0.4 * (avg_sentence_length + 100 * complex_word_ratio)\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            'vietnamese': calculate_complexity(self.article_vi.content),\n",
    "            'english': calculate_complexity(self.article_en.content)\n",
    "        }\n",
    "\n",
    "    def _extract_named_entities(self):\n",
    "        \"\"\"Trích xuất các thực thể có tên\"\"\"\n",
    "        def extract_entities(text):\n",
    "            capitalized_words = re.findall(r'\\b[A-Z][a-zA-Z]*(?:\\s+[A-Z][a-zA-Z]*)*\\b', text)\n",
    "            entities = sorted(set(capitalized_words))\n",
    "            \n",
    "            return {\n",
    "                'count': len(entities),\n",
    "                'entities': entities[:20]\n",
    "            }\n",
    "\n",
    "        return {\n",
    "            'vietnamese': extract_entities(self.article_vi.content),\n",
    "            'english': extract_entities(self.article_en.content)\n",
    "        }\n",
    "\n",
    "    def _analyze_temporal_references(self):\n",
    "        \"\"\"Phân tích các tham chiếu thời gian\"\"\"\n",
    "        def find_temporal_references(text):\n",
    "            patterns = {\n",
    "                'years': r'\\b\\d{4}\\b',\n",
    "                'dates': r'\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b',\n",
    "                'months': r'\\b(January|February|March|April|May|June|July|August|September|October|November|December)\\b'\n",
    "            }\n",
    "            \n",
    "            results = {}\n",
    "            for key, pattern in patterns.items():\n",
    "                matches = re.findall(pattern, text)\n",
    "                results[key] = {\n",
    "                    'count': len(matches),\n",
    "                    'examples': sorted(set(matches))[:10]\n",
    "                }\n",
    "                \n",
    "            return results\n",
    "\n",
    "        return {\n",
    "            'vietnamese': find_temporal_references(self.article_vi.content),\n",
    "            'english': find_temporal_references(self.article_en.content)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17879ab-cdd1-4997-8750-067fe3ef117a",
   "metadata": {},
   "source": [
    "- Lớp ContentAnalyzer có nhiệm vụ phân tích nội dung văn bản từ hai bài viết Wikipedia bằng tiếng Việt và tiếng Anh. Các phương thức chính bao gồm:\n",
    "  - analyze_text_content: Phân tích toàn bộ nội dung văn bản và trả về kết quả của các phân tích con.\n",
    "  - _analyze_word_frequency: Phân tích tần suất từ và lọc từ thông dụng dựa trên danh sách stopwords.\n",
    "  - _analyze_sentiment: Đánh giá tình cảm/quan điểm của văn bản qua độ phân cực và tính chủ quan.\n",
    "  - _analyze_readability: Đánh giá độ dễ đọc qua số câu, số từ, độ dài trung bình của câu và từ.\n",
    "  - _analyze_complexity: Phân tích độ phức tạp của văn bản, tính tỷ lệ từ phức tạp và chỉ số Gunning Fog Index.\n",
    "  - _extract_named_entities: Trích xuất các thực thể có tên (danh từ riêng, tên địa danh...).\n",
    "  - _analyze_temporal_references: Tìm kiếm các tham chiếu về thời gian như năm, ngày tháng, tên tháng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e33b0430-19b6-4af1-9ff7-b6c19e946fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructureAnalyzer:\n",
    "    def __init__(self, article_vi, article_en):\n",
    "        \"\"\"Khởi tạo bộ phân tích cấu trúc\"\"\"\n",
    "        self.article_vi = article_vi\n",
    "        self.article_en = article_en\n",
    "\n",
    "    def analyze_structure(self):\n",
    "        \"\"\"Phân tích cấu trúc bài viết\"\"\"\n",
    "        results = {\n",
    "            'sections': self._analyze_sections(),\n",
    "            'links': self._analyze_links(),\n",
    "            'citations': self._analyze_citations(),\n",
    "            'images': self._analyze_images()\n",
    "        }\n",
    "        return results\n",
    "\n",
    "    def _analyze_sections(self):\n",
    "        \"\"\"Phân tích các phần của bài viết\"\"\"\n",
    "        def get_sections(soup):\n",
    "            sections = []\n",
    "            for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):\n",
    "                level = int(heading.name[1])\n",
    "                title = heading.get_text().strip()\n",
    "                sections.append({'level': level, 'title': title})\n",
    "            return sections\n",
    "\n",
    "        return {\n",
    "            'vietnamese': get_sections(self.article_vi.soup),\n",
    "            'english': get_sections(self.article_en.soup)\n",
    "        }\n",
    "\n",
    "    def _analyze_links(self):\n",
    "        \"\"\"Phân tích các liên kết trong bài viết\"\"\"\n",
    "        def get_links(soup):\n",
    "            links = []\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link.get('href')\n",
    "                if href.startswith('/wiki/'):\n",
    "                    links.append(href)\n",
    "            return links\n",
    "\n",
    "        return {\n",
    "            'vietnamese': get_links(self.article_vi.soup),\n",
    "            'english': get_links(self.article_en.soup)\n",
    "        }\n",
    "\n",
    "    def _analyze_citations(self):\n",
    "        \"\"\"Phân tích các trích dẫn\"\"\"\n",
    "        def get_citations(soup):\n",
    "            citations = soup.find_all('cite')\n",
    "            return [cite.get_text() for cite in citations]\n",
    "\n",
    "        return {\n",
    "            'vietnamese': get_citations(self.article_vi.soup),\n",
    "            'english': get_citations(self.article_en.soup)\n",
    "        }\n",
    "\n",
    "    def _analyze_images(self):\n",
    "        \"\"\"Phân tích các hình ảnh\"\"\"\n",
    "        def get_images(soup):\n",
    "            images = []\n",
    "            for img in soup.find_all('img'):\n",
    "                images.append({\n",
    "                    'src': img.get('src', ''),\n",
    "                    'alt': img.get('alt', '')\n",
    "                })\n",
    "            return images\n",
    "\n",
    "        return {\n",
    "            'vietnamese': get_images(self.article_vi.soup),\n",
    "            'english': get_images(self.article_en.soup)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f06a5c-676b-4d6e-88ba-4be28793c5e0",
   "metadata": {},
   "source": [
    "- Lớp StructureAnalyzer phân tích cấu trúc của hai bài viết Wikipedia bằng tiếng Việt và tiếng Anh. Các phương thức chính bao gồm:\n",
    "    - analyze_structure: Phân tích cấu trúc toàn bộ bài viết, bao gồm các phần, liên kết, trích dẫn và hình ảnh.\n",
    "    - _analyze_sections: Phân tích các tiêu đề trong bài viết, lấy ra danh sách các phần với cấp độ (h1, h2, h3,...) và tiêu đề của chúng.\n",
    "    - _analyze_links: Trích xuất các liên kết (links) trong bài viết, chỉ tập trung vào các liên kết nội bộ của Wikipedia (bắt đầu bằng \"/wiki/\").\n",
    "    - _analyze_citations: Trích xuất các trích dẫn (citations) từ bài viết, lưu lại văn bản của các phần trích dẫn.\n",
    "    - _analyze_images: Phân tích các hình ảnh trong bài viết, lưu lại thông tin về nguồn ảnh (src) và mô tả ảnh (alt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5856ab80-5db1-4f22-aec1-a9d3ed22512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualizationGenerator:\n",
    "    def __init__(self, content_analysis, structure_analysis, output_dir):\n",
    "        \"\"\"Khởi tạo bộ tạo trực quan hóa\"\"\"\n",
    "        self.content_analysis = content_analysis\n",
    "        self.structure_analysis = structure_analysis\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def generate_visualizations(self):\n",
    "        \"\"\"Tạo tất cả các trực quan hóa\"\"\"\n",
    "        self._generate_word_frequency_plot()\n",
    "        self._generate_sentiment_plot()\n",
    "        self._generate_structure_plot()\n",
    "        \n",
    "        self._generate_citation_plot()\n",
    "        self._generate_readability_plot()\n",
    "\n",
    "    def _generate_word_frequency_plot(self):\n",
    "        \"\"\"Tạo biểu đồ tần suất từ\"\"\"\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        \n",
    "        # Tạo dữ liệu cho biểu đồ\n",
    "        vi_words, vi_freqs = zip(*self.content_analysis['word_frequency']['vietnamese'][:10])\n",
    "        en_words, en_freqs = zip(*self.content_analysis['word_frequency']['english'][:10])\n",
    "        \n",
    "        # Vẽ biểu đồ cột cho tiếng Việt\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.barh(vi_words, vi_freqs)\n",
    "        plt.title('Top Words (Vietnamese)')\n",
    "        plt.xlabel('Frequency')\n",
    "        \n",
    "        # Vẽ biểu đồ cột cho tiếng Anh\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.barh(en_words, en_freqs)\n",
    "        plt.title('Top Words (English)')\n",
    "        plt.xlabel('Frequency')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / 'word_frequency.png')\n",
    "        plt.close()\n",
    "\n",
    "    def _generate_sentiment_plot(self):\n",
    "        \"\"\"Tạo biểu đồ phân tích tình cảm\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Lấy dữ liệu sentiment\n",
    "        vi_sentiment = self.content_analysis['sentiment']['vietnamese']\n",
    "        en_sentiment = self.content_analysis['sentiment']['english']\n",
    "        \n",
    "        # Tạo dữ liệu cho biểu đồ\n",
    "        languages = ['Vietnamese', 'English']\n",
    "        polarity = [vi_sentiment['polarity'], en_sentiment['polarity']]\n",
    "        subjectivity = [vi_sentiment['subjectivity'], en_sentiment['subjectivity']]\n",
    "        \n",
    "        # Vẽ biểu đồ\n",
    "        x = np.arange(len(languages))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, polarity, width, label='Polarity')\n",
    "        plt.bar(x + width/2, subjectivity, width, label='Subjectivity')\n",
    "        \n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Sentiment Analysis')\n",
    "        plt.xticks(x, languages)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / 'sentiment.png')\n",
    "        plt.close()\n",
    "\n",
    "    def _generate_structure_plot(self):\n",
    "        \"\"\"Tạo biểu đồ cấu trúc\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Lấy dữ liệu cấu trúc\n",
    "        vi_sections = len(self.structure_analysis['sections']['vietnamese'])\n",
    "        en_sections = len(self.structure_analysis['sections']['english'])\n",
    "        vi_links = len(self.structure_analysis['links']['vietnamese'])\n",
    "        en_links = len(self.structure_analysis['links']['english'])\n",
    "        vi_citations = len(self.structure_analysis['citations']['vietnamese'])\n",
    "        en_citations = len(self.structure_analysis['citations']['english'])\n",
    "        vi_images = len(self.structure_analysis['images']['vietnamese'])\n",
    "        en_images = len(self.structure_analysis['images']['english'])\n",
    "        \n",
    "        # Tạo dữ liệu cho biểu đồ\n",
    "        categories = ['Sections', 'Links', 'Citations', 'Images']\n",
    "        vi_data = [vi_sections, vi_links, vi_citations, vi_images]\n",
    "        en_data = [en_sections, en_links, en_citations, en_images]\n",
    "        \n",
    "        x = np.arange(len(categories))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, vi_data, width, label='Vietnamese')\n",
    "        plt.bar(x + width/2, en_data, width, label='English')\n",
    "        \n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Article Structure Comparison')\n",
    "        plt.xticks(x, categories)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / 'structure.png')\n",
    "        plt.close()\n",
    "   \n",
    "\n",
    "    def _generate_citation_plot(self):\n",
    "        \"\"\"Tạo biểu đồ trích dẫn\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Lấy số lượng trích dẫn\n",
    "        vi_citations = len(self.structure_analysis['citations']['vietnamese'])\n",
    "        en_citations = len(self.structure_analysis['citations']['english'])\n",
    "        \n",
    "        # Vẽ biểu đồ pie\n",
    "        plt.pie([vi_citations, en_citations], \n",
    "                labels=['Vietnamese', 'English'],\n",
    "                autopct='%1.1f%%',\n",
    "                colors=['lightblue', 'lightgreen'])\n",
    "        \n",
    "        plt.title('Citation Distribution')\n",
    "        plt.axis('equal')\n",
    "        \n",
    "        plt.savefig(self.output_dir / 'citations.png')\n",
    "        plt.close()\n",
    "\n",
    "    def _generate_readability_plot(self):\n",
    "        \"\"\"Tạo biểu đồ độ dễ đọc\"\"\"\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Lấy dữ liệu độ dễ đọc\n",
    "        vi_readability = self.content_analysis['readability']['vietnamese']\n",
    "        en_readability = self.content_analysis['readability']['english']\n",
    "        \n",
    "        # Tạo dữ liệu cho biểu đồ\n",
    "        metrics = ['Sentence Count', 'Word Count', 'Avg Sentence Length', 'Avg Word Length']\n",
    "        vi_data = [vi_readability[k.lower().replace(' ', '_')] for k in metrics]\n",
    "        en_data = [en_readability[k.lower().replace(' ', '_')] for k in metrics]\n",
    "        \n",
    "        x = np.arange(len(metrics))\n",
    "        width = 0.35\n",
    "        \n",
    "        plt.bar(x - width/2, vi_data, width, label='Vietnamese')\n",
    "        plt.bar(x + width/2, en_data, width, label='English')\n",
    "        \n",
    "        plt.ylabel('Value')\n",
    "        plt.title('Readability Metrics')\n",
    "        plt.xticks(x, metrics, rotation=45)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / 'readability.png')\n",
    "        plt.close()\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b30041b-1483-4820-afeb-e7d49d152df2",
   "metadata": {},
   "source": [
    "- Lớp VisualizationGenerator tạo ra các biểu đồ trực quan để minh họa kết quả phân tích nội dung và cấu trúc của hai bài viết (tiếng Việt và tiếng Anh). Các phương thức chính bao gồm:\n",
    "  - generate_visualizations: Gọi các phương thức phụ để tạo tất cả các biểu đồ, gồm tần suất từ, phân tích tình cảm, cấu trúc bài viết, trích dẫn, và độ dễ đọc.\n",
    "  - _generate_word_frequency_plot: Tạo biểu đồ cột ngang cho top 10 từ phổ biến nhất trong hai bài viết (tiếng Việt và tiếng Anh).\n",
    "  - _generate_sentiment_plot: Tạo biểu đồ cột so sánh mức độ \"Polarity\" (tính tích cực) và \"Subjectivity\" (tính chủ quan) giữa hai bài viết.\n",
    "  - _generate_structure_plot: Tạo biểu đồ cột để so sánh số lượng phần, liên kết, trích dẫn, và hình ảnh giữa hai bài viết.\n",
    "  - _generate_citation_plot: Tạo biểu đồ tròn để so sánh số lượng trích dẫn giữa hai bài viết.\n",
    "  - _generate_readability_plot: Tạo biểu đồ cột so sánh các chỉ số về độ dễ đọc (số câu, số từ, độ dài trung bình câu, độ dài trung bình từ) giữa hai bài viết.\n",
    "- Mỗi biểu đồ đều được lưu vào thư mục đầu ra (output_dir) dưới định dạng ảnh (.png)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d286237-a699-42fd-a493-f97e77742c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReportGenerator:\n",
    "    def __init__(self, content_analysis, structure_analysis, visualizations, output_dir):\n",
    "        \"\"\"Khởi tạo bộ tạo báo cáo\"\"\"\n",
    "        self.content_analysis = content_analysis\n",
    "        self.structure_analysis = structure_analysis\n",
    "        self.visualizations = visualizations\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def generate_report(self):\n",
    "        \"\"\"Tạo báo cáo phân tích đầy đủ\"\"\"\n",
    "        report = {\n",
    "            'metadata': {\n",
    "                'generated_at': datetime.now().isoformat(),\n",
    "                'version': '1.0'\n",
    "            },\n",
    "            'content_analysis': self.content_analysis,\n",
    "            'structure_analysis': self.structure_analysis,\n",
    "            'visualizations': list(map(str, self.output_dir.glob('*.png')))\n",
    "        }\n",
    "\n",
    "        # Lưu báo cáo JSON\n",
    "        with open(self.output_dir / 'report.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(report, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # Tạo báo cáo HTML\n",
    "        self._generate_html_report(report)\n",
    "\n",
    "    def _generate_html_report(self, report):\n",
    "        \"\"\"Tạo báo cáo HTML\"\"\"\n",
    "        html_content = f\"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>Wikipedia Analysis Report</title>\n",
    "            <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css\" rel=\"stylesheet\">\n",
    "            <style>\n",
    "                .visualization-section img {{\n",
    "                    max-width: 100%;\n",
    "                    height: auto;\n",
    "                    margin: 20px 0;\n",
    "                }}\n",
    "                .metrics-section {{\n",
    "                    margin: 20px 0;\n",
    "                }}\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <div class=\"container mt-5\">\n",
    "                <h1 class=\"mb-4\">Wikipedia Analysis Report</h1>\n",
    "                \n",
    "                <div class=\"metrics-section\">\n",
    "                    <h2>Content Analysis</h2>\n",
    "                    <div class=\"card\">\n",
    "                        <div class=\"card-body\">\n",
    "                            <pre>{json.dumps(self.content_analysis, indent=2)}</pre>\n",
    "                        </div>\n",
    "                    </div>\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"metrics-section\">\n",
    "                    <h2>Structure Analysis</h2>\n",
    "                    <div class=\"card\">\n",
    "                        <div class=\"card-body\">\n",
    "                            <pre>{json.dumps(self.structure_analysis, indent=2)}</pre>\n",
    "                        </div>\n",
    "                    </div>\n",
    "                </div>\n",
    "                \n",
    "                <div class=\"visualization-section\">\n",
    "                    <h2>Visualizations</h2>\n",
    "                    {self._generate_visualization_html()}\n",
    "                </div>\n",
    "            </div>\n",
    "            \n",
    "            <script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js\"></script>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        with open(self.output_dir / 'report.html', 'w', encoding='utf-8') as f:\n",
    "            f.write(html_content)\n",
    "\n",
    "    def _generate_visualization_html(self):\n",
    "        \"\"\"Tạo HTML cho phần trực quan hóa\"\"\"\n",
    "        html = \"\"\n",
    "        for img_path in sorted(self.output_dir.glob('*.png')):\n",
    "            if img_path.name.endswith('.png'):\n",
    "                html += f\"\"\"\n",
    "                <div class=\"card mb-4\">\n",
    "                    <div class=\"card-header\">\n",
    "                        <h5 class=\"card-title\">{img_path.stem.replace('_', ' ').title()}</h5>\n",
    "                    </div>\n",
    "                    <div class=\"card-body\">\n",
    "                        <img src=\"{img_path.name}\" alt=\"{img_path.stem}\" class=\"img-fluid\">\n",
    "                    </div>\n",
    "                </div>\n",
    "                \"\"\"\n",
    "        return html "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485e3e2a-bb24-4022-b8d7-be268441c7f9",
   "metadata": {},
   "source": [
    "- generate_report:\n",
    "  - Tạo báo cáo dạng JSON chứa:\n",
    "        - Thông tin metadata về thời gian tạo báo cáo và phiên bản.\n",
    "        - Dữ liệu phân tích nội dung (content_analysis).\n",
    "        - Dữ liệu phân tích cấu trúc (structure_analysis).\n",
    "        - Danh sách các hình ảnh trực quan đã tạo.\n",
    "    - Báo cáo JSON này được lưu vào tệp report.json.\n",
    "    - Sau đó, nó gọi phương thức phụ để tạo báo cáo HTML.\n",
    "- _generate_html_report:\n",
    "  - Tạo một báo cáo HTML sử dụng Bootstrap để tạo giao diện đẹp và responsive\n",
    "  - Báo cáo bao gồm các phần:\n",
    "    - Content Analysis: Hiển thị phân tích nội dung dưới dạng JSON.\n",
    "    - Structure Analysis: Hiển thị phân tích cấu trúc dưới dạng JSON.\n",
    "    - Visualizations: Hiển thị tất cả hình ảnh trực quan dưới dạng thẻ Bootstrap.\n",
    "- _generate_visualization_html:Tạo phần HTML để hiển thị từng hình ảnh trực quan.\n",
    "- Output\n",
    "  -  report.json: Lưu trữ toàn bộ dữ liệu phân tích dưới dạng JSON.\n",
    "  -  report.html: Báo cáo HTML với nội dung phân tích và hình ảnh trực quan đi kèm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dea3fb4f-cb9d-41ec-a727-ad82f6a756e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Khởi tạo hệ thống\n",
    "    system = WikipediaAnalysisSystem()\n",
    "\n",
    "    # Tạo các đối tượng bài viết\n",
    "    article_vi = WikipediaArticle(\n",
    "        \"https://vi.wikipedia.org/wiki/Chiến_tranh_Việt_Nam\",\n",
    "        \"vietnamese\"\n",
    "    )\n",
    "    article_en = WikipediaArticle(\n",
    "        \"https://en.wikipedia.org/wiki/Vietnam_War\",\n",
    "        \"english\"\n",
    "    )\n",
    "\n",
    "    # Lấy nội dung\n",
    "    if not article_vi.fetch_content() or not article_en.fetch_content():\n",
    "        logger.error(\"Failed to fetch content for one or both articles\")\n",
    "        return\n",
    "\n",
    "    # Phân tích nội dung\n",
    "    content_analyzer = ContentAnalyzer(article_vi, article_en, system)\n",
    "    content_analysis = content_analyzer.analyze_text_content()\n",
    "\n",
    "    # Phân tích cấu trúc\n",
    "    structure_analyzer = StructureAnalyzer(article_vi, article_en)\n",
    "    structure_analysis = structure_analyzer.analyze_structure()\n",
    "\n",
    "    # Tạo trực quan hóa\n",
    "    visualization_generator = VisualizationGenerator(\n",
    "        content_analysis,\n",
    "        structure_analysis,\n",
    "        system.output_dir\n",
    "    )\n",
    "    visualization_generator.generate_visualizations()\n",
    "\n",
    "    # Tạo báo cáo\n",
    "    report_generator = ReportGenerator(\n",
    "        content_analysis,\n",
    "        structure_analysis,\n",
    "        visualization_generator,\n",
    "        system.output_dir\n",
    "    )\n",
    "    report_generator.generate_report()\n",
    "\n",
    "    logger.info(\"Analysis completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17dc34b-6d51-4415-a441-d47fc29bd026",
   "metadata": {},
   "source": [
    "- Hàm main() kết hợp tất cả các bước từ việc thu thập nội dung, phân tích, tạo trực quan hóa, đến việc tạo báo cáo tổng hợp chi tiết."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e49727f0-9db5-43a3-adce-845ca2de3f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\84388\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\84388\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "2024-10-22 15:43:27,436 - __main__ - INFO - NLTK resources downloaded successfully\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\84388\\anaconda3\\Lib\\logging\\__init__.py\", line 1163, in emit\n",
      "    stream.write(msg + self.terminator)\n",
      "  File \"C:\\Users\\84388\\anaconda3\\Lib\\encodings\\cp1252.py\", line 19, in encode\n",
      "    return codecs.charmap_encode(input,self.errors,encoding_table)[0]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "UnicodeEncodeError: 'charmap' codec can't encode character '\\u1ebf' in position 80: character maps to <undefined>\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\84388\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\84388\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\84388\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\84388\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\84388\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 322, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"C:\\Users\\84388\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\84388\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\84388\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\84388\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\84388\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\84388\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\84388\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\84388\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\84388\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\84388\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\84388\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\84388\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\84388\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\84388\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\84388\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\84388\\AppData\\Local\\Temp\\ipykernel_13664\\1417491173.py\", line 3, in <module>\n",
      "    main()\n",
      "  File \"C:\\Users\\84388\\AppData\\Local\\Temp\\ipykernel_13664\\3787799472.py\", line 16, in main\n",
      "    if not article_vi.fetch_content() or not article_en.fetch_content():\n",
      "  File \"C:\\Users\\84388\\AppData\\Local\\Temp\\ipykernel_13664\\88092672.py\", line 19, in fetch_content\n",
      "    logger.info(f\"Successfully fetched content for {self.title}\")\n",
      "Message: 'Successfully fetched content for Chiến tranh Việt Nam'\n",
      "Arguments: ()\n",
      "2024-10-22 15:43:28,679 - __main__ - INFO - Successfully fetched content for Chiến tranh Việt Nam\n",
      "2024-10-22 15:43:30,141 - __main__ - INFO - Successfully fetched content for Vietnam War\n",
      "2024-10-22 15:43:34,156 - __main__ - INFO - Analysis completed successfully\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during execution: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c66375f-6ef2-4ac0-8968-9913b3136ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
